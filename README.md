# API Proxy

Proyecto para implementar un proxy que act√∫e como intermediario entre el API de Mercado Libre y los servidores cliente.

## Tabla de contenido
- [Problem√°tica](#problem√°tica)
- [Metas](#metas)
- [Visi√≥n general](#visi√≥n-general)
- [Dise√±o](#dise√±o)
- [Implementaci√≥n](#implementaci√≥n)
  - [Instalaci√≥n](#instalaci√≥n)
  - [Configuraci√≥n](#configuraci√≥n)
  - [Estructura del proyecto](#estructura-del-proyecto)
  - [Migraciones](#migraciones)
  - [Ejecuci√≥n](#ejecuci√≥n)
- [Escalabilidad](#escalabilidad)
- [Notas](#notas)

## Problem√°tica
Mercado Libre corre sus aplicaciones en m√°s de 20,000 servidores, los cuales suelen comunicarse entre s√≠ a trav√©s de API's y solo algunas son accesibles desde el exterior.

Uno de los problemas que se tienen actualmente es c√≥mo controlar y medir estas interconexiones. Para lo cual, se necesita crear e implementar un *proxy de API's*.

## Metas
- Permitir ejecutar la funci√≥n de proxy sobre el dominio api.mercadolibre.com: realizar la petici√≥n al proxy y este debe retornar el llamado a api.mercadolibre.com.
  - Ejemplo: `curl 127.0.0.1:8080/categories/MLA97994` deber√° retornar el contenido de `https://api.mercadolibre.com/categories/MLA97994`
- Controlar la cantidad m√°xima de llamados por:
  - IP de origen
  - Path de destino
  - Combinaciones de ambos
  - Cuota de peticiones
- Se deben almacenar estad√≠sticas de uso del proxy
- El proxy deber√° poder ejecutarse sobre Linux
- La carga media del proxy debe superar los 50,000 request/segundo

## Visi√≥n general
Esta es un API proxy que permite realizar las peticiones al API de Mercado Libre (api.mercadolibre.com) mediante el proxy y retornando la respuesta por parte del API.

El proyecto se centra en una soluci√≥n que escalar√° con el tiempo a medida que las necesidades del negocio crezcan y se necesite de una mayor disponibilidad y escalabilidad del API proxy. Para lo cual se considera un dise√±o de la arquitectura inicial.

## Dise√±o
El proyecto consiste en un proxy API que recibe las peticiones de distintos clientes para realizar la petici√≥n al API de mercadolibre, con el fin de retornar la respuesta del API.

El proxy est√° dise√±ado para recibir peticiones y mantener un rendimiento estable para su carga media de 50,000 request/segundo. El proxy debe poder ser ejecutado en un servidor Linux, para lo cual se utilizar√° el administrador de procesos para Node.js, PM2, el cual permitir√° que el servicio se mantenga en ejecuci√≥n y permitiendo que se genere un registro de logs b√°sico en caso de que surjan errores al momento en el que el servicio presente latencias o una carga excesiva.

El proxy, al haberse pensado para correr en servidores Linux, puede ser llevado a cualquier proveedor en la nube que permita la ejecuci√≥n del proyecto seg√∫n las especificaciones de la implementaci√≥n. Algunos proveedores podr√≠an ser Google Cloud Platform, Amazon Web Services, Microsoft Azure, Cloudflare Workers, etc. La elecci√≥n del proveedor depender√° de acuerdo a c√≥mo vaya escalando el proyecto.

![Diagrama de arquitectura](https://github.com/jessicaramsa/api-proxy-ml/blob/main/resources/imgs/architecture.png?raw=true)

Al recibir las peticiones de los distintos clientes, de manera paralela se piensa llevar el registro de las estad√≠sticas de uso que se le da al API. Este proceso en paralelo deber√° llevar el registro de la IP de origen, el path de destino, informaci√≥n completa del payload de la request (headers, m√©todo, body, etc.), adicionalmente, se deber√° llevar el registro de la cantidad de peticiones que se han realizado desde la misma IP de origen hacia el path de destino del API para realizar el conteo de la cuota de peticiones por la combinaci√≥n de la IP de origen y el path de destino.

La informaci√≥n generada para las estad√≠sticas de uso se guardar√° en una base de datos relacional, para lo cual se utilizar√° una base de datos MySQL. Esto con la finalidad de que en un futuro se pueda llegar a escalar el proyecto y explotar los beneficios que ofrecen las bases de datos relacionales y llevarlo a un cluster de BigTable.

Con el fin de evitar problemas con las consultas hacia la base de datos, se implementar√° la indexaci√≥n de las tablas existentes permitiendo que los datos sean filtrados con mayor facilidad y eficiencia. La indexaci√≥n resultar√° importante en la fase inicial y a corto plazo, dando tiempo para analizar la cantidad de registros que ser√°n almacenados y posteriormente, procesados para mostrar las estad√≠sticas de uso.

![Diagrama de base de datos](https://github.com/jessicaramsa/api-proxy-ml/blob/main/resources/imgs/database.png?raw=true)

El siguiente diagrama muestra las clases que se utilizan en el proyecto.

![Diagrama de clases](https://github.com/jessicaramsa/api-proxy-ml/blob/main/resources/imgs/classes.png?raw=true)

En este diagrama se encuentran las clases que identifican a las request que llegar√°n al proxy, las cuales tendr√°n sus respectivos atributos y m√©todos. A su vez, la clase que identifica las request se comunicar√° con la clase de la m√©trica de uso ya que depender√° del n√∫mero de peticiones que lleguen al proxy y con base en esa m√©trica ser√° conforme se pueda calcular el uso del proxy.

Podr√≠an existir m√°s clases que se puedan generar a partir del uso del proxy, sin embargo, se deja fuera del alcance.

Las tecnolog√≠as utilizadas:
- Javascript
- [Node.js v16.14.2 (o superior, recomendado)](https://nodejs.org/en/download/)
- [PM2](https://pm2.keymetrics.io/)
- Database MySQL

## Implementaci√≥n

### Instalaci√≥n
1. Clonar el repositorio via HTTPS o SSH (recomendado)
```bash
git clone git@github.com:jessicaramsa/api-proxy-ml.git
```

2. Verificar que la versi√≥n de Node.js sea la recomendada o una superior
```bash
node -v
```

3. Instalar globalmente PM2 para correr el proceso del proyecto
```bash
npm install pm2 -g
```

4. Instalar las dependencias del proyecto
```bash
npm install
```

### Configuraci√≥n
1. Copiar el template del archivo con las variables de entorno
```bash
cp .env.example .env
```

2. Cambiar la configuraci√≥n del puerto seg√∫n sea conveniente para ejecutar el proxy

### Ejecuci√≥n
1. Ejecutar el proyecto mediante npm
```bash
npm start
```

2. Levantar el proceso mediante PM2
```bash
pm2 start ecosystem.config.js
pm2 show api-proxy
```

### Estructura del proyecto
```
üì¶api-proxy-ml
 ‚î£ üìÇresources
 ‚îÉ ‚îó üìÇimgs
 ‚îÉ ‚îÉ ‚î£ üìúarchitecture.png
 ‚îÉ ‚îÉ ‚î£ üìúclasses.png
 ‚îÉ ‚îÉ ‚îó üìúscalability.png
 ‚î£ üì¶test
 ‚îÉ ‚î£ üìúmakeRequests.sh
 ‚îÉ ‚îó üìúrequests.txt
 ‚î£ üìÇsrc
 ‚îÉ ‚îó üìúindex.js
 ‚î£ üìú.env
 ‚î£ üìú.env.example
 ‚î£ üìú.gitignore
 ‚î£ üìúLICENSE
 ‚î£ üìúpackage.json
 ‚îó üìúREADME.md
```

## Escalabilidad
La soluci√≥n actual se enfoca en la funcionalidad principal para realizar el proxy, sin embargo, se puede llegar a escalar la soluci√≥n a medida que las necesidades del negocio crezcan, adem√°s, de tener una arquitectura mucho m√°s robusta.

El proxy podr√≠a recibir una carga de request/segundo que pueda llegar a un punto en el que se presente demasiada latencia, o bien, que el servicio presente ca√≠das a medida que la cantidad de request/segundo vaya incrementando. Por lo que se piensa que a mediano plazo, se escale el proyecto a una arquitectura m√°s robusta donde permita que la funci√≥n del proxy ya existente se lleve a contenedores Docker.

Esto permitir√° que por una parte la implementaci√≥n se pueda llevar con facilidad a otro servidor y se pueda implementar una soluci√≥n escalable de alta disponibilidad en el servicio a largo plazo. Como una de las primeras ventajas que ofrecer√≠a implementar el contenedor, ser√≠a la seguridad de la aplicaci√≥n ya que se estar√≠a aislando el proyecto del resto de procesos que est√©n ejecut√°ndose en el servidor donde se despliegue. Independientemente del proceso que se despliegue, el contenedor se encargar√° de la ejecuci√≥n del proxy y ser√° una eficiencia en los tiempos de despliegue y configuraci√≥n.

Una de las cosas a agregar, antes de que el resto de servidores se comuniquen con el proxy, ser√° un balanceador de cargas para el servidor que puede ser implementado de distintas maneras dependiendo el proveedor en la nube que se utilice para realizar el host de la aplicaci√≥n. Es importante considerar este balanceador de cargas en momentos en los que la aplicaci√≥n necesite una alta disponibilidad.

Al igual que el servidor host para el proxy, se podr√≠a implementar un balanceador de cargas para el cluster de la base de datos. Este balanceador de cargas se encargar√° de distribuir la carga hacia la base de datos de acuerdo a la cantidad de transacciones que se realicen y/o el n√∫mero de conexiones hacia la base de datos.

![Diagrama de arquitectura escalable](https://github.com/jessicaramsa/api-proxy-ml/blob/main/resources/imgs/scalability.png?raw=true)

Las conexiones de escritura hacia la base de datos representar√≠an una baja carga, por lo que, las conexiones de lectura se someter√≠an a monitoreo que permita decidir la cantidad de r√©plicas que se podr√≠an llegar a manejar para tener una alta disponibilidad de los datos. Adem√°s, permitir√≠a que las estad√≠sticas de uso se puedan mostrar en la debida interfaz sin generar cuellos de botella por consumir altos vol√∫menes de datos.
